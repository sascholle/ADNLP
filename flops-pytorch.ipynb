{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3ad8a4",
   "metadata": {},
   "source": [
    "- <https://kipp.ly/blog/transformer-inference-arithmetic/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cda7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0.dev20221031'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1a18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils._pytree import tree_map, tree_flatten\n",
    "from typing import List, Any\n",
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "from torch.utils._python_dispatch import TorchDispatchMode\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "# Source: https://pastebin.com/V3wATa7w, Horace He\n",
    "aten = torch.ops.aten\n",
    "\n",
    "def get_shape(i):\n",
    "    return i.shape\n",
    "\n",
    "def prod(x):\n",
    "    res = 1\n",
    "    for i in x:\n",
    "        res *= i\n",
    "    return res\n",
    "\n",
    "def matmul_flop(inputs: List[Any], outputs: List[Any]) -> Number:\n",
    "    \"\"\"\n",
    "    Count flops for matmul.\n",
    "    \"\"\"\n",
    "    # Inputs should be a list of length 2.\n",
    "    # Inputs contains the shapes of two matrices.\n",
    "    input_shapes = [get_shape(v) for v in inputs]\n",
    "    assert len(input_shapes) == 2, input_shapes\n",
    "    assert input_shapes[0][-1] == input_shapes[1][-2], input_shapes\n",
    "    flop = prod(input_shapes[0]) * input_shapes[-1][-1]\n",
    "    return flop\n",
    "\n",
    "def addmm_flop(inputs: List[Any], outputs: List[Any]) -> Number:\n",
    "    \"\"\"\n",
    "    Count flops for fully connected layers.\n",
    "    \"\"\"\n",
    "    # Count flop for nn.Linear\n",
    "    # inputs is a list of length 3.\n",
    "    input_shapes = [get_shape(v) for v in inputs[1:3]]\n",
    "    # input_shapes[0]: [batch size, input feature dimension]\n",
    "    # input_shapes[1]: [batch size, output feature dimension]\n",
    "    assert len(input_shapes[0]) == 2, input_shapes[0]\n",
    "    assert len(input_shapes[1]) == 2, input_shapes[1]\n",
    "    batch_size, input_dim = input_shapes[0]\n",
    "    output_dim = input_shapes[1][1]\n",
    "    flops = batch_size * input_dim * output_dim\n",
    "    return flops\n",
    "\n",
    "def bmm_flop(inputs: List[Any], outputs: List[Any]) -> Number:\n",
    "    \"\"\"\n",
    "    Count flops for the bmm operation.\n",
    "    \"\"\"\n",
    "    # Inputs should be a list of length 2.\n",
    "    # Inputs contains the shapes of two tensor.\n",
    "    assert len(inputs) == 2, len(inputs)\n",
    "    input_shapes = [get_shape(v) for v in inputs]\n",
    "    n, c, t = input_shapes[0]\n",
    "    d = input_shapes[-1][-1]\n",
    "    flop = n * c * t * d\n",
    "    return flop\n",
    "\n",
    "def conv_flop_count(\n",
    "    x_shape: List[int],\n",
    "    w_shape: List[int],\n",
    "    out_shape: List[int],\n",
    "    transposed: bool = False,\n",
    ") -> Number:\n",
    "    \"\"\"\n",
    "    Count flops for convolution. Note only multiplication is\n",
    "    counted. Computation for addition and bias is ignored.\n",
    "    Flops for a transposed convolution are calculated as\n",
    "    flops = (x_shape[2:] * prod(w_shape) * batch_size).\n",
    "    Args:\n",
    "        x_shape (list(int)): The input shape before convolution.\n",
    "        w_shape (list(int)): The filter shape.\n",
    "        out_shape (list(int)): The output shape after convolution.\n",
    "        transposed (bool): is the convolution transposed\n",
    "    Returns:\n",
    "        int: the number of flops\n",
    "    \"\"\"\n",
    "    batch_size = x_shape[0]\n",
    "    conv_shape = (x_shape if transposed else out_shape)[2:]\n",
    "    flop = batch_size * prod(w_shape) * prod(conv_shape)\n",
    "    return flop\n",
    "\n",
    "def conv_flop(inputs: List[Any], outputs: List[Any]):\n",
    "    \"\"\"\n",
    "    Count flops for convolution.\n",
    "    \"\"\"\n",
    "    x, w = inputs[:2]\n",
    "    x_shape, w_shape, out_shape = (get_shape(x), get_shape(w), get_shape(outputs[0]))\n",
    "    transposed = inputs[6]\n",
    "\n",
    "    return conv_flop_count(x_shape, w_shape, out_shape, transposed=transposed)\n",
    "\n",
    "def transpose_shape(shape):\n",
    "    return [shape[1], shape[0]] + list(shape[2:])\n",
    "\n",
    "def conv_backward_flop(inputs: List[Any], outputs: List[Any]):\n",
    "    grad_out_shape, x_shape, w_shape = [get_shape(i) for i in inputs[:3]]\n",
    "    output_mask = inputs[-1]\n",
    "    fwd_transposed = inputs[7]\n",
    "    flop_count = 0\n",
    "\n",
    "    if output_mask[0]:\n",
    "        grad_input_shape = get_shape(outputs[0])\n",
    "        flop_count += conv_flop_count(grad_out_shape, w_shape, grad_input_shape, not fwd_transposed)\n",
    "    if output_mask[1]:\n",
    "        grad_weight_shape = get_shape(outputs[1])\n",
    "        flop_count += conv_flop_count(transpose_shape(x_shape), grad_out_shape, grad_weight_shape, fwd_transposed)\n",
    "\n",
    "    return flop_count\n",
    "\n",
    "\n",
    "flop_mapping = {\n",
    "    aten.mm: matmul_flop,\n",
    "    aten.matmul: matmul_flop,\n",
    "    aten.addmm: addmm_flop,\n",
    "    aten.bmm: bmm_flop,\n",
    "    aten.convolution: conv_flop,\n",
    "    aten._convolution: conv_flop,\n",
    "    aten.convolution_backward: conv_backward_flop,\n",
    "}\n",
    "\n",
    "def normalize_tuple(x):\n",
    "    if not isinstance(x, tuple):\n",
    "        return (x,)\n",
    "    return x\n",
    "\n",
    "class FlopCounterMode(TorchDispatchMode):\n",
    "    def __init__(self, model = None):\n",
    "        self.flop_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.parents = ['Global']\n",
    "        if model is not None:\n",
    "            for name, module in dict(model.named_children()).items():\n",
    "                module.register_forward_pre_hook(self.enter_module(name))\n",
    "                module.register_forward_hook(self.exit_module(name))\n",
    "\n",
    "    def enter_module(self, name):\n",
    "        def f(module, inputs):\n",
    "            self.parents.append(name)\n",
    "            inputs = normalize_tuple(inputs)\n",
    "            out = self.create_backwards_pop(name)(*inputs)\n",
    "            return out\n",
    "\n",
    "        return f\n",
    "\n",
    "    def exit_module(self, name):\n",
    "        def f(module, inputs, outputs):\n",
    "            assert(self.parents[-1] == name)\n",
    "            self.parents.pop()\n",
    "            outputs = normalize_tuple(outputs)\n",
    "            return self.create_backwards_push(name)(*outputs)\n",
    "        return f\n",
    "\n",
    "    def create_backwards_push(self, name):\n",
    "        class PushState(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, *args):\n",
    "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
    "                if len(args) == 1:\n",
    "                    return args[0]\n",
    "                return args\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, *grad_outs):\n",
    "                self.parents.append(name)\n",
    "                return grad_outs\n",
    "\n",
    "        return PushState.apply\n",
    "\n",
    "    def create_backwards_pop(self, name):\n",
    "        class PopState(torch.autograd.Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, *args):\n",
    "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
    "                if len(args) == 1:\n",
    "                    return args[0]\n",
    "                return args\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, *grad_outs):\n",
    "                assert(self.parents[-1] == name)\n",
    "                self.parents.pop()\n",
    "                return grad_outs\n",
    "\n",
    "        return PopState.apply\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.flop_counts.clear()\n",
    "        super().__enter__()\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f\"Total: {sum(self.flop_counts['Global'].values())/1e9 } GFLOPS\")\n",
    "        for mod in self.flop_counts.keys():\n",
    "            print(f\"Module: \", mod)\n",
    "            for k,v in self.flop_counts[mod].items():\n",
    "                print(f\"{k}: {v/1e9} GFLOPS\")\n",
    "            print()\n",
    "        super().__exit__(*args)\n",
    "\n",
    "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
    "        kwargs = kwargs if kwargs else {}\n",
    "\n",
    "        out = func(*args, **kwargs)\n",
    "        func_packet = func._overloadpacket\n",
    "        if func_packet in flop_mapping:\n",
    "            flop_count = flop_mapping[func_packet](args, normalize_tuple(out))\n",
    "            for par in self.parents:\n",
    "                self.flop_counts[par][func_packet] += flop_count\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74375ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the below from this tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12254e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 20_000  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916830c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.8 GFLOPS\n",
      "Module:  Global\n",
      "aten.bmm: 0.8 GFLOPS\n",
      "\n",
      "Module:  transformer_encoder\n",
      "aten.bmm: 0.8 GFLOPS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randint(1, 10, (20, 1000), device='mps') # fake inputs from random integers\n",
    "src_mask = generate_square_subsequent_mask(20).to('mps') # mask\n",
    "flop_counter = FlopCounterMode(model)\n",
    "with flop_counter:\n",
    "    model(inputs, src_mask).sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7f8fb",
   "metadata": {},
   "source": [
    "On PyTorch 1.12.1, the following should work:\n",
    "(By Sam Andow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537190d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.utils._python_dispatch import push_torch_dispatch_mode\n",
    "from functools import partial\n",
    "\n",
    "inp = torch.randn((8, 3, 224, 224), device='cuda')\n",
    "mod = models.resnet18().cuda()\n",
    "with FlopCounterMode.push(mod) as flop_counter1:\n",
    "    mod(inp).sum().backward()\n",
    "\n",
    "with FlopCounterMode.push(mod) as flop_counter2:\n",
    "    mod(inp).sum().backward()\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
